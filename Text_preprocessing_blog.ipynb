{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing ##\n",
    "\n",
    "Tutorial ini membahas basic class untuk preprocessing text.\n",
    "\n",
    "Misalkan kita memiliki list teks, misalnya daftar quotes top film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   #                                              QUOTE               MOVIE  \\\n",
      "0  1             Frankly, my dear, I don't give a damn.  GONE WITH THE WIND   \n",
      "1  2       I'm gonna make him an offer he can't refuse.       THE GODFATHER   \n",
      "2  3  You don't understand!  I coulda had class. I c...   ON THE WATERFRONT   \n",
      "3  4  Toto, I've a feeling we're not in Kansas anymore.    THE WIZARD OF OZ   \n",
      "4  5                        Here's looking at you, kid.          CASABLANCA   \n",
      "\n",
      "   YEAR                     Director  \n",
      "0  1939          Victor Fleming       \n",
      "1  1972    Francis Ford Coppola       \n",
      "2  1954              Elia Kazan       \n",
      "3  1939          Victor Fleming       \n",
      "4  1942          Michael Curtiz       \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"movie_quotes.csv\", encoding='utf-8')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kita akan membangun class yang membersihkan/cleans quotes dan menghasilkan encoding one-hot atau matriks TF-IDF.\n",
    "\n",
    "Biasanya, data teks dibersihkan dengan menghapus elemen yang tidak diperlukan. Misalnya, kata-kata seperti saya, aku, gw, kamu, muncul dalam banyak teks dan hampir tidak informatif. Ini disebut stopwords. Kuta harus menghapus ini. Kedua, untuk membuat corpus teks yang lebih homogen, kita bisa menurunkan huruf kecil semua karakter sehingga misal kata \"Sintha sayangku\" identik dengan \"sintha sayangku\". Terakhir, dalam tutorial ini kita akan stem kata-kata tersebut ke bentuk akarnya. Ini berarti kita akan mengubah kata-kata seperti memancing,dan kepancing menjadi akar kata pancing.\n",
    "\n",
    "Class dalam Python mirip dengan membuat pemotong kue. Kita ingin membuat prototipe agar setiap kita menggunakan cookie cutter pada dataset teks, objek tersebut akan memiliki bentuk, karakteristik, dan fungsi yang sama.\n",
    "\n",
    "Mari kita mulai dengan menginisialisasi class kita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "class Preprocess():\n",
    "\tdef __init__(self, text, sw=stopwords.words('english'), lower=True, stem = True):\n",
    "\n",
    "\t\tif not (type(text)==pd.core.series.Series):\n",
    "\t\t\ttext = pd.Series(text)\n",
    "\n",
    "\t\tself.text = text\n",
    "\t\tself.sw = sw\n",
    "\t\tself.lower = lower\n",
    "\t\tself.stem = stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setiap class pada Python harus <i>diinisialisasi</i>. Pada dasarnya, kita ingin menggunakan fungsi <b>$__init__$</b> (yaitu dua garis bawah sebelum dan setelah kata init) untuk memberi objek beberapa karakteristik dasar. Dalam kasus kita, ini akan menjadi karakteristik bagaimana kita ingin membersihkan/cleaning data.\n",
    "\n",
    "<b>self</b> adalah elemen penting dari program berorientasi objek. Ini adalah class itu sendiri dan akan menjadi variabel yang dapat kita tambahkan atributnya.\n",
    "\n",
    "<b>text</b> akan menjadi daftar teks yang ingin kita bersihkan. Variabel ini wajib dimasukkan oleh pengguna saat memanggil kelas Preprocess yang kita buat.\n",
    "\n",
    "<b>sw</b> adalah daftar stopwords. Kamu akan melihat ini sama dengan nilai (stopwords.words ('english')). Ini berarti secara default stopwords akan diimpor dari modul nltk (alat bahasa alami yang bagus yang dibuat oleh orang-orang baik di UPenn <a>http://www.nltk.org</a>). Ini berarti sw TIDAK wajib karena akan mengambil nilai secara default jika pengguna tidak memberinya nilai.\n",
    "\n",
    "<b>lower</b> adalah variabel Boolean secara default sama dengan True. Kita akan menggunakan ini nanti untuk mengetahui apakah teks harus huruf kecil atau tidak.\n",
    "\n",
    "<b>stem</b>  adalah variabel Boolean secara default sama dengan True. Kita akan menggunakan ini nanti untuk mengetahui apakah akan stem kata-kata dalam teks atau tidak.\n",
    "\n",
    "Baris terakhir seperti <i>self.text = text</i> adalah atribut pegging ke objek kita. Artinya, di tempat lain di dalam kelas, kita dapat merujuk ke atribut ini dengan menjalankan/running <i>class_instance</i>.text.\n",
    "\n",
    "(type (text) == pd.core.series.Series) memeriksa apakah teks tersebut adalah seri Pandas/pandas series. Jika bukan, maka kita akan mengconvert menjadi satu. Pandas adalah modul yang memudahkan untuk memanipulasi list, misalnya menurunkan casing atau mengembalikan dataframe.\n",
    "\n",
    "Berikut contohnya:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               Frankly, my dear, I don't give a damn.\n",
      "1         I'm gonna make him an offer he can't refuse.\n",
      "2    You don't understand!  I coulda had class. I c...\n",
      "3    Toto, I've a feeling we're not in Kansas anymore.\n",
      "4                          Here's looking at you, kid.\n",
      "Name: QUOTE, dtype: object\n",
      "True\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "docs = Preprocess(df.QUOTE)\n",
    "\n",
    "print(docs.text.head())\n",
    "print(docs.lower)\n",
    "print(docs.sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama kita membuat <i>instance</i> kelas, lalu kita memanggil atributnya (teks yang saya cetak hanya bagian atas, atau head, dari dataframe untuk menghemat ruang).\n",
    "\n",
    "Sekarang kita memiliki fungsi $__ init __$, mari kita definisikan <b>method</b> pertama kita. Method adalah fungsi di dalam class. Argumen pertama harus self, karena variabel ini berisi atribut yang kita definisikan dalam fungsi $__ init __$. Pertama, kita akan memeriksa apakah atribut <i>lower</i> adalah True; jika demikian, mari kita kecilkan teksnya. Kemudian, mari kita pisahkan setiap teks menjadi list kata. Dengan cara ini kita dapat menggilir setiap kata dan \n",
    "i) stem jika self.stem True \n",
    "ii) Menghapusnya.remove jika itu stopword. \n",
    "\n",
    "Mirip dengan fungsi Python lainnya, kita dapat mendefinisikan fungsi di dalam fungsi untuk membuat kode yang lebih bersih. Terakhir, kita akan menggabungkan setiap list kata untuk membuat string, dan kemudian menginisialisasi objek TfidfVectorizer. TfidfVectorizer akan memungkinkan kita membuat matriks padat di mana setiap kolom adalah kata dalam kosakata kita, dan setiap baris sesuai dengan dokumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "\tdef __init__(self, text, sw=stopwords.words('english'), lower=True, stem = True):\n",
    "\n",
    "\t\tif not (type(text)==pd.core.series.Series):\n",
    "\t\t\ttext = pd.Series(text)\n",
    "\n",
    "\t\tself.text = text\n",
    "\t\tself.sw = sw\n",
    "\t\tself.lower = lower\n",
    "\t\tself.stem = stem\n",
    "\n",
    "\n",
    "\tdef clean_text(self):\n",
    "\t\tdef stem(word_list):\n",
    "\t\t\treturn map(lambda x: PorterStemmer().stem(x), word_list)\n",
    "\n",
    "\t\tdef remove_sw(word_list):\n",
    "\t\t\tkeep = []\n",
    "\t\t\tfor word in word_list:\n",
    "\t\t\t\tif not word in self.sw:\n",
    "\t\t\t\t\tkeep.append(word)\n",
    "\t\t\treturn keep\n",
    "\n",
    "\t\tif self.lower:\n",
    "\t\t\tself.text = self.text.str.lower()\n",
    "\n",
    "\t\tself.text = self.text.apply(lambda x: x.split())\n",
    "\t\t\n",
    "\t\tif self.stem: self.text = self.text.apply(stem)\n",
    "\t\tif self.sw: self.text = self.text.apply(remove_sw)\n",
    "\n",
    "\t\tself.text = self.text.apply(lambda x: ' '.join(x))\n",
    "\t\tself.vectorizer = TfidfVectorizer()\n",
    "\t\tself.df_dense = self.vectorizer.fit_transform(self.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                            frankly, dear, give damn.\n",
      "1                   i'm gonna make offer can't refuse.\n",
      "2    understand! coulda class. coulda contender. co...\n",
      "3                   toto, i'v feel we'r kansa anymore.\n",
      "4                                 here' look you, kid.\n",
      "Name: QUOTE, dtype: object\n",
      "TfidfVectorizer()\n",
      "  (0, 70)\t0.478079705674071\n",
      "  (0, 111)\t0.5209988435904572\n",
      "  (0, 73)\t0.478079705674071\n",
      "  (0, 104)\t0.5209988435904572\n",
      "  (1, 235)\t0.5008150769034725\n",
      "  (1, 49)\t0.407581696306535\n",
      "  (1, 209)\t0.5008150769034725\n",
      "  (1, 175)\t0.407581696306535\n",
      "  (1, 115)\t0.407581696306535\n"
     ]
    }
   ],
   "source": [
    "docs = Preprocess(df.QUOTE)\n",
    "docs.clean_text()\n",
    "\n",
    "print(docs.text.head())\n",
    "print(docs.vectorizer)\n",
    "print(docs.df_dense[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atribut <b>df_dense</b> adalah representasi dense dari matriks TF-IDF (<a>https://en.wikipedia.org/wiki/Tf%E2%80%93idf).</a>. (0, 70) sesuai dengan dokumen pertama, \"frankly, dear, I don't give a damn.\", Dan kata ke-71 dalam kosakata kita yang terkandung dalam kutipan itu. 0.478079... adalah skor TF-IDF.\n",
    "\n",
    "Kita mungkin ingin mengembalikan ini dalam bentuk kumpulan data yang lebih mudah dibaca. Jadi mari kita buat dataframe Pandas di mana kolomnya adalah kata-kata dalam kosakata kita setelah membersihkan teks, self.vectorizer.get_feature_names(), dan kemudian nilai yang bisa kita dapatkan dari self.df_dense.toarray().\n",
    "\n",
    "<b>onehot</b> akan menjadi variabel yang dapat kita gunakan sehingga pengguna dapat mengembalikan matriks dengan nilai 1 jika dokumen tersebut berisi kata dalam kosa kata di posisi ke-i, atau skor TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  adrian  again  ahead  \\\n",
      "0                          frankly, dear, give damn.     0.0    0.0    0.0   \n",
      "1                 i'm gonna make offer can't refuse.     0.0    0.0    0.0   \n",
      "2  understand! coulda class. coulda contender. co...     0.0    0.0    0.0   \n",
      "3                 toto, i'v feel we'r kansa anymore.     0.0    0.0    0.0   \n",
      "4                               here' look you, kid.     0.0    0.0    0.0   \n",
      "\n",
      "   ain  airplanes  alive  all  alone  alway  ...  win  wire  witness  word  \\\n",
      "0  0.0        0.0    0.0  0.0    0.0    0.0  ...  0.0   0.0      0.0   0.0   \n",
      "1  0.0        0.0    0.0  0.0    0.0    0.0  ...  0.0   0.0      0.0   0.0   \n",
      "2  0.0        0.0    0.0  0.0    0.0    0.0  ...  0.0   0.0      0.0   0.0   \n",
      "3  0.0        0.0    0.0  0.0    0.0    0.0  ...  0.0   0.0      0.0   0.0   \n",
      "4  0.0        0.0    0.0  0.0    0.0    0.0  ...  0.0   0.0      0.0   0.0   \n",
      "\n",
      "   world   ya  yet   yo  you  youngster  \n",
      "0    0.0  0.0  0.0  0.0  0.0        0.0  \n",
      "1    0.0  0.0  0.0  0.0  0.0        0.0  \n",
      "2    0.0  0.0  0.0  0.0  0.0        0.0  \n",
      "3    0.0  0.0  0.0  0.0  0.0        0.0  \n",
      "4    0.0  0.0  0.0  0.0  1.0        0.0  \n",
      "\n",
      "[5 rows x 334 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "# download('stopwords')\n",
    "\n",
    "df = pd.read_csv(\"movie_quotes.csv\", encoding='utf-8')\n",
    "\n",
    "class Preprocess():\n",
    "\tdef __init__(self, text, sw=stopwords.words('english'), lower=True, stem = True):\n",
    "\n",
    "\t\tif not (type(text)==pd.core.series.Series):\n",
    "\t\t\ttext = pd.Series(text)\n",
    "\n",
    "\t\tself.text = text\n",
    "\t\tself.sw = sw\n",
    "\t\tself.lower = lower\n",
    "\t\tself.stem = stem\n",
    "\n",
    "\n",
    "\tdef clean_text(self):\n",
    "\t\tdef stem(word_list):\n",
    "\t\t\treturn map(lambda x: PorterStemmer().stem(x), word_list)\n",
    "\n",
    "\t\tdef remove_sw(word_list):\n",
    "\t\t\tkeep = []\n",
    "\t\t\tfor word in word_list:\n",
    "\t\t\t\tif not word in self.sw:\n",
    "\t\t\t\t\tkeep.append(word)\n",
    "\t\t\treturn keep\n",
    "\n",
    "\t\tif self.lower:\n",
    "\t\t\tself.text = self.text.str.lower()\n",
    "\n",
    "\t\tself.text = self.text.apply(lambda x: x.split())\n",
    "\t\t\n",
    "\t\tif self.stem: self.text = self.text.apply(stem)\n",
    "\t\tif self.sw: self.text = self.text.apply(remove_sw)\n",
    "\n",
    "\t\tself.text = self.text.apply(lambda x: ' '.join(x))\n",
    "\t\tself.vectorizer = TfidfVectorizer()\n",
    "\t\tself.df_dense = self.vectorizer.fit_transform(self.text)\n",
    "\n",
    "\tdef array(self, onehot=1):\n",
    "\t\tarray = self.df_dense.toarray().copy()\n",
    "\t\tif onehot:\n",
    "\t\t\tarray[array>0] = 1\n",
    "\t\treturn array\n",
    "\n",
    "\tdef make_df(self,onehot=1):\n",
    "\t\tdf = pd.DataFrame(columns=self.vectorizer.get_feature_names(),\n",
    "\t\t\t\t\t\t\tdata = self.array(onehot))\n",
    "\t\tdf['Text'] = self.text\n",
    "\t\tdf = df[['Text']+list(df.columns[:-1])]\n",
    "\t\treturn df\n",
    "\n",
    "docs = Preprocess(df.QUOTE)\n",
    "docs.clean_text()\n",
    "text_df = docs.make_df(onehot=1)\n",
    "\n",
    "print(text_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil diatas kita akan coba convert dalam bentuk csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv (r'export_dataframe.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
